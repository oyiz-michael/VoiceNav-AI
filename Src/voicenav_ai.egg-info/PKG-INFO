Metadata-Version: 2.4
Name: voicenav-ai
Version: 1.0.0
Summary: Serverless voice-controlled accessibility for the web
Author: VoiceNav-AI Contributors
License: MIT
Project-URL: Homepage, https://github.com/oyiz-michael/VoiceNav-AI
Project-URL: Repository, https://github.com/oyiz-michael/VoiceNav-AI
Project-URL: Issues, https://github.com/oyiz-michael/VoiceNav-AI/issues
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: boto3>=1.34.0
Requires-Dist: botocore>=1.34.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: moto>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Requires-Dist: safety>=2.0.0; extra == "dev"
Requires-Dist: bandit>=1.7.0; extra == "dev"
Dynamic: license-file

# VoiceNav-AI ğŸ¤

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AWS](https://img.shields.io/badge/AWS-Serverless-orange)](https://aws.amazon.com/)
[![Python](https://img.shields.io/badge/Python-3.12-blue)](https://python.org)

> **Serverless voice-controlled accessibility for the web**

VoiceNav-AI is a serverless accessibility tool that enables hands-free web navigation through natural voice commands. Built entirely on AWS serverless infrastructure, it converts speech to structured intents using Amazon Transcribe and AWS Bedrock, delivering real-time actions to any web application.

ğŸ† **Winner - AWS Lambda Hackathon 2025**

## âœ¨ Features

- ğŸ¯ **Zero Installation**: Works on any website without browser extensions
- ğŸ—£ï¸ **Natural Voice Commands**: "Click book appointment", "Navigate to contact"
- âš¡ **Real-time Processing**: WebSocket-based instant response
- ğŸ’° **Cost Effective**: Serverless architecture idles at ~$0.18/month
- â™¿ **Accessibility First**: Designed for users with motor skill impairments
- ğŸ”’ **Secure**: No persistent storage of audio data

## ğŸš€ Demo

Try the live demo: [staging.d1uaa8nlpc4ipt.amplifyapp.com](https://staging.d1uaa8nlpc4ipt.amplifyapp.com/)

**Voice commands to try:**
- "Book appointment"
- "Click contact support"  
- "Navigate to home"

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    A[Browser MediaRecorder] -->|PUT audio| S3[(S3 audio-store/)]
    S3 -->|ObjectCreated event| T[Lambda: Transcribe Trigger]
    T --> TR[Amazon Transcribe]
    TR -->|JSON transcript| S3out[(S3 transcribe-output/)]
    S3out -->|ObjectCreated event| B[Lambda: Bedrock Processor]
    B -->|Invoke Claude-3| BR[AWS Bedrock]
    BR -->|JSON intent| B
    B -->|Scan connections| D[(DynamoDB)]
    B -->|PostToConnection| WS[API Gateway WebSocket]
    WS -->|Real-time intent| A
```

### Components

1. **Frontend (Client/)**: Vanilla JavaScript SPA with WebSocket client
2. **Lambda Functions (Src/)**:
   - **StoreConn**: WebSocket connection management
   - **Transcribe Processor**: Triggers transcription jobs
   - **Bedrock Processor**: Intent analysis and broadcasting
3. **Infrastructure**: S3, DynamoDB, API Gateway, Lambda, Transcribe, Bedrock

## ğŸ› ï¸ Getting Started

### Prerequisites

- AWS Account with appropriate permissions
- AWS CLI configured
- Node.js (for local development)
- Python 3.12 (for Lambda functions)

### Quick Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/oyiz-michael/VoiceNav-AI.git
   cd VoiceNav-AI
   ```

2. **Deploy Infrastructure**
   ```bash
   # Option 1: Use AWS CDK (recommended)
   cd infrastructure
   npm install
   cdk deploy

   # Option 2: Manual deployment (see docs/DEPLOYMENT.md)
   ```

3. **Configure Environment**
   ```bash
   # Update Client/config.js with your AWS endpoints
   cp Client/config.example.js Client/config.js
   # Edit config.js with your values
   ```

4. **Test Locally**
   ```bash
   cd Client
   python -m http.server 8000
   # Open http://localhost:8000
   ```

5. **Try the Integration Example**
   ```bash
   # Open the example in your browser
   open examples/integration-example.html
   ```

## ğŸ“– Usage

### Integration with Any Website

Add VoiceNav-AI to any website in 3 steps:

1. **Include the library**:
   ```html
   <script src="https://cdn.jsdelivr.net/gh/oyiz-michael/VoiceNav-AI@main/dist/voicenav.min.js"></script>
   ```

2. **Initialize with your config**:
   ```javascript
   const voiceNav = new VoiceNav({
     wsUrl: 'wss://your-api-gateway-url',
     region: 'us-east-1',
     bucket: 'your-s3-bucket'
   });
   ```

3. **Define voice commands**:
   ```javascript
   voiceNav.addCommands({
     'book appointment': '#book-btn',
     'contact us': '#contact-link',
     'go to home': '#home-nav'
   });
   ```

### Supported Actions

- **click**: Click any element by selector
- **navigate**: Navigate to URLs or hash routes
- **type**: Fill form fields with voice input
- **scroll**: Scroll page up/down
- **focus**: Focus on specific elements

## ğŸ§ª Development

### Project Structure

```
VoiceNav-AI/
â”œâ”€â”€ Client/                 # Frontend web application
â”‚   â”œâ”€â”€ index.html         # Demo website
â”‚   â”œâ”€â”€ app.js            # Core VoiceNav client
â”‚   â”œâ”€â”€ style.css         # Demo styles
â”‚   â””â”€â”€ config.js         # Configuration
â”œâ”€â”€ Src/                   # Lambda functions
â”‚   â”œâ”€â”€ store_conn/       # WebSocket connection handler
â”‚   â”œâ”€â”€ transcribe_processor/  # Audio transcription trigger
â”‚   â””â”€â”€ bedrock_processor/     # AI intent processing
â”œâ”€â”€ infrastructure/        # CDK deployment code
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ tests/                # Test suites
```

### Running Tests

```bash
# Use Makefile for comprehensive testing
make test                    # Run all tests
make test-py                 # Python tests only
make quick-test             # Quick test without JS dependencies

# Or run directly:
# Frontend tests
cd Client && npm test

# Lambda function tests
cd Src && python -m pytest

# Integration tests
npm run test:integration
```

### Development Workflow

```bash
# Complete development setup
make setup

# Code quality checks
make lint                    # Lint code
make type-check             # Type checking
make format                 # Format code
make check                  # All quality checks

# Development servers
make dev-client             # Start client dev server
make dev-docs              # Serve documentation

# AWS operations
make deploy                 # Deploy to AWS
make logs                   # Tail CloudWatch logs
make status                 # Check AWS resources

# Validation
make validate-structure     # Validate Python package structure
make validate-config        # Check configuration files
```

### Environment Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `REGION` | AWS region | `us-east-1` |
| `AWS_BUCKET` | S3 bucket name | `voicenav-bucket` |
| `CONN_TABLE` | DynamoDB table | `VoiceNavConnections` |
| `WS_ENDPOINT` | WebSocket endpoint | `wss://api.execute-api...` |
| `MODEL_ID` | Bedrock model ID | `anthropic.claude-3-sonnet...` |

## ğŸ“Š Performance

- **Cold Start**: ~200ms average
- **Transcription Latency**: ~45s (async processing)
- **Intent Processing**: ~1-2s
- **WebSocket Delivery**: <100ms
- **Monthly Cost**: ~$0.18 (idle), scales with usage

## ğŸ”’ Security

- **No Audio Storage**: Audio files auto-deleted after processing
- **CORS Protection**: Configured for specific origins
- **IAM Roles**: Least-privilege access patterns
- **WebSocket TTL**: Automatic connection cleanup
- **Input Validation**: All user inputs sanitized

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Quick Contribution Steps

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **AWS Lambda Hackathon 2025** - For the inspiration and platform
- **Team Uncommon Grounds Network** - For collaborative development
- **AWS Bedrock & Claude-3** - For powerful AI intent processing
- **Web Accessibility Initiative** - For accessibility guidelines

## ğŸ“ Support

- ğŸ“š [Documentation](docs/)
- ğŸ› [Issue Tracker](https://github.com/oyiz-michael/VoiceNav-AI/issues)
- ğŸ’¬ [Discussions](https://github.com/oyiz-michael/VoiceNav-AI/discussions)
- ğŸ“§ [Email Support](mailto:support@voicenav-ai.com)

## ğŸ—ºï¸ Roadmap

- [ ] **Streaming Transcription**: Reduce latency to <3s
- [ ] **Multi-language Support**: Support for 20+ languages  
- [ ] **ARIA Integration**: Auto-generate selectors from ARIA roles
- [ ] **Chrome Extension**: Browser extension for any website
- [ ] **Mobile SDK**: Native mobile app integration
- [ ] **Analytics Dashboard**: Usage analytics and insights

---

**Built with â¤ï¸ for web accessibility**
